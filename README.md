# Sounds Localized with Acoustic Monitoring (SLAM)
## Dataset formatting standard v1.0

This document describes a dataset standard for packaging up a set of audio clips and metadata that describe sound events and associated spatial positions. Typically, this data is generated by deploying audio recorders with precise time-synchronization capabilities (e.g. AudioMoth with on-board GPS), then post-processing audio to detect and estimate spatial positions of sound events. 

The key components are (1) a table describing the time and location of the acoustic events (`localized_events.csv`), (2) a set of audio files with subfolders for each recorder (`/audio/`), and (3) metadata providing the spatial positions of each recorder (/localization\_metadata/point\_table.csv). 

This standard lives on [the SLAM github repo](https://github.com/sammlapp/SLAM)

If you use this protocol, add your dataset to our list of SLAM-formatted datasets by either

(a) forking this repository, adding a link to your dataset in the dataset_list.md file, and creating a Pull Request 

or (b) emailing one of the repository managers (currently, Sam Lapp) with the details of your dataset to be added

## File structure:

`/project_name/`  
	`readme.md`  
	`localized_events.csv`  
	`classes.csv [optional]`  
	`/script/`  
`scripts/notebooks: .py, .ipynb, .r, .rmd`  
		`environment.yml`   
	`/localization_metadata/`  
		`audio_file_table.csv`  
		`point_table.csv`  
	`/optional: observed_events/`  
		`playbacks.csv`  
		`observations.csv`  
	`/audio/ (within this folder, structure is flexible)`  
		`/recorder_id001/`  
			`/synchronized_audio_file_001.wav`  
			`…`  
		`…`


Metadata contains a set of tables \- as in a “database”, some fields have to key into other tables as exact matches:

- `recorder_id` maps between the audio\_file\_table and point\_table  
- `file_id`: maps between audio\_file\_table and localized\_events “file\_ids” column

Audio files are assumed to be “synchronized” in the sense that temporal correction data has been used to correct for clock drift. Varying levels of precision are required depending on the localization use case. 

The organization of audio files within /audio/ folder is flexible. Each audio file will have a unique file\_id, and the audio\_file\_table.csv links the file\_id with its relative path and the point\_id where it was recorded.

### Terminology:

- `Event`: a sound produced in the world and recorded on multiple microphones
- `Synchronized`: audio files have been post-processed so that they achieve precisely the nominal sample rate and start at precisely the expected time

## Top-level files

### Event table (`localized_events.csv`)

Columns:

* event\_id: unique within the dataset; fixed length; only alphanumeric characters and underscores  
  * Examples: `pennsylvania2024grid1_00001`, `event01000` , `0001`  
* label: species or other sound type name (e.g. “playback”) of the localized event  
  * Matches a value in `classes.csv` class column  
* Start\_timestamp: onset time of the event in ISO format `2025-05-20T10:00:00.000-05:00`  
* duration: event length in seconds  
* position: localized position coordinate in meters: (utm x, utm y ,elevation, utm zone)  
* file\_ids: list of file\_id  for audio clips that participated in the localization  
  * should match a value in the `file_id` column of `audio_file_table.csv`   
  * Alternatively, can list all file\_id on which the event is detectable, even if the file did not participate in the localization of the event  
* file\_start\_time\_offsets: time in seconds from start of each audio file to the the start of the clip 

	(for example, the event might start 10 seconds in to file1.wav and 15 seconds in to file2.wav) 

**Optional fields** (these are not generated as a single value in correlation-sum approaches):

* tdoas: List of relative time of arrival in seconds: one per file\_id  
  * Arbitrary reference point (often, tdoa=0 is based on the arrival time at one microphone)  
* distance\_residuals: List of residual distance error: one per file\_id, in meters  
* classifier\_scores: List of classifier score (float): one per file\_id  
* \[which optional fields are useful for correlation-sum method?\]  
  * Some sort of uncertainty measurement?

Can add additional Boolean/categorical columns that describe validation checks performed on the event; e.g. species\_manually\_confirmed: True/False; 

### Descriptive ReadMe File (`readme.md`)

Text file describing the purpose and design of the study and the procedure used to generate localizations, perform species detection, and synchronize the audio files.

See the readme_template.md in this repository for a template to follow. Rename the file to simply `readme.md` in your dataset.

### [optional] Classes (sound types) description (`classes.csv`)

This optional file lists and describes all classes (sound types) that occur in the “label” column of localized\_events.csv. For instance, “Scarlet Tanager alarm call” might be one class. It can optionally include other columns that assist in the interpretation of the class or detail its taxonomy. If not included, be sure to describe the meaning of the values in “label” column of localized\_events.csv in the README.md file. 

Columns:

- `class`: name of the species/class of the localized event, exactly matches values in “label” column of localized\_events.csv  
- \[optional\] `description`: textual description of the class, including information on inclusion/exclusion rules  
- Optionally, include other fields. For instance `scientific_name`, `genus`, or `common_name`


## Scripts `subfolder (script/)`

Code scripts and notebooks (R, python,  etc) used to generate detections and localizations

* If there is no hand-labeling or manual review of clips: should be able to re-create localized\_events.csv from these scripts  
* If there is a hand-labeling/manual review step: include any scripts/workflows used to review events, e.g. if you used an interactive notebook, if you produced a table of events to exclude/include, etc.

### Environment file(s) 
Frozen Python environment files (.yml) listing package versions

> Tip: If using the Anaconda environment manager, you can create this file with `conda env export -f myenv.yml` 

## Localization metadata `subfolder (localization_metadata/)`

### Point table (`localization_metadata/point_table.csv`)  

**Columns**: point\_id | utm\_easting | utm\_northing | elevation | utm\_zone 

Tabulates the locations of all microphones. The coordinates (utm\_e, utm\_n, elevation, utm\_zone) refer to the position of the microphone; UTM coordinates have units of meters. Sometimes RTK provides two elevation measurements: Ellipsoidal\_Ht and Ortho\_Ht; in this case it is recommended to retain both as additional columns

\[optional\] array : must include this column if multiple arrays are included in the localization dataset. Each array should have a unique alpha-numeric ID containing only letters, numbers, spaces, underscores 

\[optional\] accuracy columns along 3 axes (east-west, north-south, elevation): | accuracy\_ew | accuracy\_ns | accuracy\_elev: These are sometimes provided by RTK GPS. They can be an estimated value if you have not recorded it for each position. Readme should describe the accuracy metric or how accuracy was measured/approximated

The readme should describe how coordinates were determined; eg hardware used, correctional data and processing, and methodology.

recorder\_id column values match values in audio\_file\_table recorder\_id column.

Additional columns such as notes, otho\_ht, ellipsoidal\_ht, can be included. 

### Audio file table (`localization_metadata/audio_file_table.csv`)  
file\_id | relative\_path  | point\_id  | start\_timestamp (ISO)

- The relative path provides the path to the audio file relative to the top-level of the dataset, e.g. `/audio/recorder001/clip101.wav`  
- point\_id matches a value of point\_id in the point\_table.csv, the location where the audio file was recorded

### Project metadata and supplementary files

These should be **sufficient to reproduce** the event\_table.csv from the contents of localization\_metadata and /audio/ (up to some human participation in the process).

## Observed events `optional subfolder (observed_events/)`

Subfolder containing records of events seen (or produced) in person with position estimates (accuracy of position estimates should go in README). All of these are optional. 

### Acoustic playback experiments (`observed_events/playbacks.csv`)

Columns:

* playback\_id: unique within the dataset; fixed length; only alphanumeric characters and underscores  
* class\_label: species or other class name (e.g. “tone sweep”, “species mix”) of the playback  
* start\_timestamp:  onset time of playback in ISO format `2025-05-20T10:00:00.000-05:00`  
* duration: playback length in seconds  
* position: playback position coordinate in UTM/meters: (utm x, utm y ,elevation, utm zone)

### Field observations with positions (`observed_events/observations.csv`)
table of human field observation of acoustic events with known positions

Columns:

* observed\_event\_id: unique within the dataset; fixed length; only alphanumeric characters and underscores  
* class\_label: species or other class name (e.g. “songsparrow\_aggression”) of the localized event  
* Start\_timestamp: onset time of event in ISO format `2025-05-20T10:00:00.000-05:00`  
* duration: event length in seconds  
* position: sound source position coordinate in meters: (utm x, utm y ,elevation, utm zone)  
* comments  
* \[optional\]: direction (degrees CW from N): orientation of animal when sound was produced

### Other observation records
Optionally, include files for other observation types, such as:

- Spot mapping  
- Point count survey  
- Transect survey  
- Focal follows

Include the cleaned data files and any supplemental metadata (typically as a .csv or .json). In the README.md file, describe how the data were generated (protocol, post-processing) and how to interpret the files in this folder (e.g. meaning of each column). If different species codes are used than in localized\_events.csv “class” column, make sure to include information on the codes and how to interpret them.
