# Sounds Localized with Acoustic Monitoring (SLAM)
## Dataset formatting standard v0.2

This document describes a dataset standard for packaging up a set of audio clips and metadata that describe sound events and associated spatial positions. Typically, this data is generated by deploying audio recorders with precise time-synchronization capabilities (e.g. AudioMoth with on-board GPS), then post-processing audio to detect and estimate spatial positions of sound events. 

The key components are (1) a table describing the time and location of the acoustic events (`localized_events.csv`), (2) a set of audio files with subfolders for each recorder (`/audio/`), and (3) metadata providing the spatial positions of each recorder (/localization\_metadata/point\_table.csv). 

This standard lives on [the SLAM github repo](https://github.com/sammlapp/SLAM)

If you use this protocol, add your dataset to our list of SLAM-formatted datasets by either
(a) forking this repository, adding a link to your dataset in the dataset_list.md file, and creating a Pull Request 
(b) emailing one of the repository managers (currently, Sam Lapp) with the details of your dataset to be added

## File structure:

`/project_name/`  
	`readme.md`  
	`localized_events.csv`  
	`classes.csv [optional]`  
	`/script/`  
`scripts/notebooks: .py, .ipynb, .r, .rmd`  
		`environment.yml`   
	`/localization_metadata/`  
		`audio_file_table.csv`  
		`point_table.csv`  
	`/optional: observed_events/`  
		`playbacks.csv`  
		`observations.csv`  
	`/audio/ (within this folder, structure is flexible)`  
		`/recorder_id001/`  
			`/synchronized_audio_file_001.wav`  
			`…`  
		`…`

`Terminology:`

- `Event: a sound produced in the world and recorded on multiple microphones`  
- `Synchronized: audio files have been post-processed so that they achieve precisely the nominal sample rate and start at precisely the expected time`

Metadata contains a set of tables \- as in a “database”, some fields have to key into other tables as exact matches:

- `recorder_id` maps between the audio\_file\_table and point\_table  
- `file_id`: maps between audio\_file\_table and localized\_events “file\_ids” column

Audio files are assumed to be “synchronized” in the sense that temporal correction data has been used to correct for clock drift. Varying levels of precision are required depending on the localization use case. 

Organization of audio files within /audio/ folder is flexible. Each audio file will have a unique file\_id, and the audio\_file\_table.csv links the file\_id with its relative path and the point\_id where it was recorded.

## Top-level files

`Event table (localized_events.csv)`

Columns:

* event\_id: unique within the dataset; fixed length; only alphanumeric characters and underscores  
  * Examples: `pennsylvania2024grid1_00001`, `event01000` , `0001`  
* label: species or other sound type name (e.g. “playback”) of the localized event  
  * Matches a value in `classes.csv` class column  
* Start\_timestamp: onset time of the event in ISO format `2025-05-20T10:00:00.000-05:00`  
* duration: event length in seconds  
* position: localized position coordinate in meters: (utm x, utm y ,elevation, utm zone)  
* file\_ids: list of file\_id  for audio clips that participated in the localization  
  * should match a value in the `file_id` column of `audio_file_table.csv`   
  * Alternatively, can list all file\_id on which the event is detectable, even if the file did not participate in the localization of the event  
* file\_start\_time\_offsets: time in seconds from start of each audio file to the the start of the clip 

	(for example, the event might start 10 seconds in to file1.wav and 15 seconds in to file2.wav) 

**Optional fields** (these are not generated as a single value in correlation-sum approaches):

* tdoas: List of relative time of arrival in seconds: one per file\_id  
  * Arbitrary reference point (often, tdoa=0 is based on the arrival time at one microphone)  
* distance\_residuals: List of residual distance error: one per file\_id, in meters  
* classifier\_scores: List of classifier score (float): one per file\_id  
* \[which optional fields are useful for correlation-sum method?\]  
  * Some sort of uncertainty measurement?

Can add additional Boolean/categorical columns that describe validation checks performed on the event; e.g. species\_manually\_confirmed: True/False; 

`Descriptive ReadMe File (readme.md)`

Text file describing the purpose and design of the study and the procedure used to generate localizations, perform species detection, and synchronize the audio files.

`[optional] Classes (sound types) description (classes.csv)`

This optional file lists and describes all classes (sound types) that occur in the “label” column of localized\_events.csv. For instance, “Scarlet Tanager alarm call” might be one class. It can optionally include other columns that assist in the interpretation of the class or detail its taxonomy. If not included, be sure to describe the meaning of the values in “label” column of localized\_events.csv in the README.md file. 

Columns:

- `class`: name of the species/class of the localized event, exactly matches values in “label” column of localized\_events.csv  
- \[optional\] `description`: textual description of the class, including information on inclusion/exclusion rules  
- Optionally, include other fields. For instance `scientific_name`, `genus`, or `common_name`

### template for readme.md:  (move this to a separate doc)

\#\# Dataset\_title

- Creators: \[fill in\]  
  - Include email for corresponding author  
- Affiliations: \[fill in\]   
- Version number \[v1\] (update any time the contents change)  
- DOI: 

\#\#\# General characteristics

- audio format: 10 second clips centered on 3 second localized events  
- dimensions localized: 2  
- number of localization arrays: 13  
- array geometry: 5x10 square grid with 33m spacing  
- sounds localized: Ovenbird songs  
- number of localization arrays: 13  
- number of audio files: 4025  
- size: 0.375 GB  
- dimensions localized: 2

Personnel 

audio\_format common approaches:: 

- long: audio files from a consistent, pre-determined time periods (e.g. 10 minutes at 9am on June 1\) are extracted from all recorders. The localized events occur within these audio files.   
- short: the included audio clips have been extracted from each localized event for each recorder (or a subset)

\#\# Study description

Study purpose: describe the purpose of the study, original intended use of the data if known, any species intended to be targeted

Personell: who led, managed, and participated in the study

Data types collected: \[List the types of data collected, e.g.: audio recordings, point counts, focal follows, individual bird positions, spot mapping, playback data\]

Notes:

- Anything else about the study site, for instance, if a particular species or behavior was observed that is relevant to the analysis  
- Describe anything noteworthy or unusual that should be considered during analysis of this data. 

\#\# Files  
This section describes the contents of each file (or file type when multiple files of the same type are included). Default descriptions can be copied if applicable. Add descriptions for any additional file types, e.g. if you include a vegetation survey or a spotmapping survey. We provide examples here

`localized_events.csv: table with a row describing each acoustically localized sound`   
	`Columns:`

* `event_id: unique within the dataset`  
* `label: species or other sound type name (e.g. “playback”) of the localized event`  
  * `Matches a value in classes.csv class column, or a class described in readme if classes.csv is not included`  
* `Start_timestamp: onset time of the event in ISO format 2025-05-20T10:00:00.000-05:00`  
* `duration: event length in seconds`  
* `position: localized position coordinate in meters: (utm x, utm y ,elevation, utm zone)`  
* `file_ids: list of file_id  [for audio clips that participated in the localization] OR [within [100m] of the event]`  
  * `matches file_id column of audio_file_table.csv`   
* `file_start_time_offsets: time in seconds from start of each audio file to the the start of the clip` 

`classes.csv: table of species names and alpha codes.`  
`Columns:`

- ``class: e.g. alpha code using IBP 2025 taxonomy [link]; these values are used in `label` column of localized_events.csv``  
- `species: English common name using IBP taxonomy`  
- `description: notes on classification, e.g. “did not annotate drumming, only vocalizations”`

`/script/sync.js: script using Open Acoustic Devices Audiomoth-Utils lib to post-process gps-enabled Audiomoth recordings`

`/script/detect.py: python script using HawkEars to detect species in audio`

`/script/localize.ipynb: python notebook using OpenSoundscape to perform acoustic localization on the outputs of /script/detect.py`

`/script/plot.rmd: R markdown script to visualize the localized songs`

`` /script/python_environment.yml: conda enviornment file; to re-create Python environment used here, run `conda create -f python_environment.yml` ``

`/localization_metadata/audio_file_table.csv: lists all audio files by audio_file_id, rel_path the relative path from the top-level directory of this dataset to the audio clip, and the point_id indexing into point_table for the position of the microphone where this audio file was recorded.`

`/localization_metadata/point_table.csv: lists the positions of each microphone`

`/audio/: each subfolder in /audio/ contains audio clips from one microphone, with an associated position listed in /localization_metadata/point_table.csv. Audio files in sub-folders are [10 second audio clips for each localized event that includes the microphone, centered on the 3-second window where the event occurred] OR [10 minutes of audio starting at 9am on 2025-01-01, 2025-01-08, and 2025-01-15].`

\#\# Sites

List and describe field site(s) where data were collected: 

- Site name, State/Province, Country  
- Ecosystem description (e.g. ecosystem type, vegetation attributes, animal community type)

\#\# Hardware

- Recorder source: \[company, e.g. “Wildlife Acoustics,” “Open Acoustic Devices,” or custom\]  
- Recorder model: \[make, model, and version of hardware and any customizations, e.g. “AudioMoth 1.2.0 with GPS receiver”\]  
- Firmware version: \[e.g. AudioMoth GPS Firmare 1.0; AudioMoth Firmware 12.1\]

\#\# Recording properties

\[These properties should describe the contents of the final dataset included in the archive; e.g. if you included only  a subset of recordings taken from a longer deployment, describe the subset\]

Date range of data: \[YYYY-MM-DD\] to \[YYYY-MM-DD\] 

Recording schedule description:

- Times of recording: \[Describe times of day recorded (specific times, or relative to sunrise/sunset)\]  
- Sleep-wake schedule: \[Recorder on-off schedule  within times of day recorded, if applicable, e.g. “10 min/20 min off”\]  
- Sample rate: \[XXXX\] Hz  
- Other relevant settings: \[e.g. gain\]

Data aggregation notes:

- If applicable, include details of how data was moved from  individual recorders onto computers and organized, what data if any was removed, what data was missing or corrected, and any manipulations. 

\#\# Recorder positioning

Placement

- Spatial pattern or geometry: \[describe general shape of microphone deployment and any intentional 3D structure, e.g. grid of hexagons, tetrahedral microphones, etc.\]  
- Range of spacing between adjacent mics: \[e.g. 30m between mics in grid, 30m between central point of hexagon and corner of hexagon\]  
- Dimensions of array: \[describe the spatial extent of the array, e.g. 250 m x 250 m\]

Deployment:  
Describe any particular decisions relevant to the deployment, for instance:

- General info/size of what the recorders were deployed on (E.g. top of 3m poles, on trees between 10-20cm DBH)  
- Particular protective housing or hardware used to affix recorders to trees/poles  
- Special deployment instructions, e.g. if devices were deployed facing north,   
- If recorders were deployed sequentially at the same points multiple times, or batteries/SD cards in the recorders were refreshed, explain the sequence here. 

Position measurement

- Method of measurement: \[describe method and the make/model of any specialized hardware used e.g.  RTK GPS, Hemisphere S631 GNSS with base station and rover; measured height difference between each pair of ARUs using meter stick or laser range finder\]  
- Measurement postprocessing: \[describe steps you took to postprocess locations, e.g. postprocessed position estimates to correct for position of RTK rover\]  
- General accuracy (lat/long): \[write a range of accuracy, e.g. “standard deviation of position measurement ranged between 0.10 m \- 0.40 m\]  
- General accuracy (elevation): \[write a range of accuracy\]  
- Other notes: \[e.g. “Position of the base station was near the north of the array; rover did not always connect correctly at the south side of the array so these recorders’ positions may have been measured less accurately”

\#\# Synchronization  
Files in archived are presumed to be synchronized (i.e. they start at precisely the time listed in the metadata and their sample rate precisely matches the audio file sample rate).

Synchronization type: \[cable, acoustic, GPS, wifi network\]

Synchronization frequency: \[describe how often the synchronization occurred, how frequently timestamps taken, e.g. “GPS timestamp taken at the beginning and end of every 10min long recording”\]

Synchronization method: 

- Methods used: \[describe any post-processing required to synchronize clips. Are there blank spaces included in files to account for buffer overflows?\]  
- Recording start/end time trimming: Synchronized recordings taken simultaneously \[WERE/WERE NOT\] trimmed to the same start/end time.  
- OPTIONAL: \[Scripts/resources: point to any particular scripts used to sync the data\]

\#\# Sound detection:

This section describes the contents of all files in the /scripts/ subdirectory. It should be possible to reproduce the localized\_events.csv table using the scripts and resources provided. This section should step through how 

Audio preprocessing: 

- Describe any resampling, noise reduction processes and algorithms used, or other pre-processing applied to the audio  
  - Are recordings in the dataset denoised? \[Do the recordings in this dataset come with denoising already applied?\]  
- Scripts: \[list filenames of relevant scripts included in this archive\]

- Describe the list of classes (sound types) that were localized. For instance, we analyzed songs but not calls of \[list of 5 bird species\]; or, 4 Blue Jay call types described by Someone et al 2024 \[1\]  
  - If the classes are drawn from a standard convention, e.g. “IBP alpha codes v 2024,” mention that convention here  
- Detection strategy: \[e.g. convolutional neural network, hand-labeling\]  
- Detector name and version: \[e.g. BirdNET v1.0\]  
- Link to detector information: \[any pertinent links to software or publications for this detector\]  
- Post-processing detector outputs:   
  - Describe binarization/thresholding of classifier scores: \[method used to generate 0-1 detection from continuous detector outputs, if applicable, e.g., thresholds used on a per-species basis\]  
  - Describe any manual review process used, e.g. confirmed species ID  
- Scripts/resources: \[list filenames of relevant scripts or resources (e.g. table of per-species thresholds) included in this archive\]

\#\# Localization:

- Tools/packages used for localization  
- Localization algorithm: \[e.g. SoundFinder, correlation-sum, custom tool\]  
- Time delay calculation algorithm: \[e.g. GCC audio cross-correlation\]  
- References: \[any pertinent links to software or publications of this algorithm\]  
- Error rejection parameters: \[any automated approaches to error rejection, e.g. minimum cross-correlation, minimum distance between recorders\]  
- Other customizations to the localization approach  
- Scripts: \[list filenames of relevant scripts included in this archive\]

\#\#\# Manual review:

- Describe any manual review performed during or after acoustic localization, and what proportion of the data underwent manual review. For example, checked alignment of cross-correlation, visually inspected heatmap of estimated position, or confirmed species ID  
- Scripts/resources: \[list filenames of relevant scripts or resource documents, e.g. tables of hand-labeled data, included in this archive\]

\#\# Observational data  
Optional, describe point counts, focal follows, spot mapping, etc

\#\# Acknowledgements  
List people and roles, eg, a list of field technicians who assisted in deployments  
Describe data ownership and any restrictions on the use of the data

\#\# License  
Include a name and link to a standard or custom license for this dataset. E.g., CC0 “no rights reserved” [https://creativecommons.org/public-domain/cc0/](https://creativecommons.org/public-domain/cc0/) 

\#\# Work Cited and Links  
References to any manuscripts or documents describing the study, results, or protocols related to this document

Link to any associated studies, protocols, reference documents, taxonomies,  or resources, with short descriptions

## 

## Scripts `subfolder (script/)`

`All scripts used to produce the localized events (.py, .r, .rmd, .ipynb)`  
Code scripts and notebooks (R, python,  etc) used to generate detections and localizations

* If there is no hand-labeling or manual review of clips: should be able to re-create localized\_events.csv from these scripts  
* If there is a hand-labeling/manual review step: include any scripts/workflows used to review events, e.g. if you used an interactive notebook, if you produced a table of events to exclude/include, etc.

`Environment file(s)`  
Frozen Python environment files (.yml) listing package versions

- If using the anaconda environment manager you can create this file with `conda env export -f myenv.yml` 

## Localization metadata `subfolder (localization_metadata/)`

`Point table (localization_metadata/point_table.csv)`  
Columns: point\_id | utm\_easting | utm\_northing | elevation | utm\_zone   
Tabulates the locations of all microphones. The coordinates (utm\_e, utm\_n, elevation, utm\_zone) refer to the position of the microphone; UTM coordinates have units of meters. Sometimes RTK provides two elevation measurements: Ellipsoidal\_Ht and Ortho\_Ht; in this case it is recommended to retain both as additional columns

\[optional\] array : must include this column if multiple arrays are included in the localization dataset. Each array should have a unique alpha-numeric ID containing only letters, numbers, spaces, underscores 

\[optional\] accuracy columns along 3 axes (east-west, north-south, elevation): | accuracy\_ew | accuracy\_ns | accuracy\_elev: These are sometimes provided by RTK GPS. They can be an estimated value if you have not recorded it for each position. Readme should describe the accuracy metric or how accuracy was measured/approximated

The readme should describe how coordinates were determined; eg hardware used, correctional data and processing, and methodology.

recorder\_id column values match values in audio\_file\_table recorder\_id column.

Additional columns such as notes, otho\_ht, ellipsoidal\_ht, can be included. 

`Audio file table (localization_metadata/audio_file_table.csv)`  
file\_id | relative\_path  | point\_id  | start\_timestamp (ISO)

- The relative path provides the path to the audio file relative to the top-level of the dataset, e.g. `/audio/recorder001/clip101.wav`  
- point\_id matches a value of point\_id in the point\_table.csv, the location where the audio file was recorded

`Project metadata and supplementary files`  
These should be **sufficient to reproduce** the event\_table.csv from the contents of localization\_metadata and /audio/ (up to some human participation in the process).

## Observed events `optional subfolder (ovserved_events/)`

## 

Subfolder containing records of events seen (or produced) in person with position estimates (accuracy of position estimates should go in README). All of these are optional. 

`Acoustic playback experiments (bserved_events/playbacks.csv)`

Columns:

* playback\_id: unique within the dataset; fixed length; only alphanumeric characters and underscores  
* class\_label: species or other class name (e.g. “tone sweep”, “species mix”) of the playback  
* start\_timestamp:  onset time of playback in ISO format `2025-05-20T10:00:00.000-05:00`  
* duration: playback length in seconds  
* position: playback position coordinate in UTM/meters: (utm x, utm y ,elevation, utm zone)

`Field observations with positions (observed_events/observations.csv)`  
table of human field observation of acoustic events with known positions

Columns:

* observed\_event\_id: unique within the dataset; fixed length; only alphanumeric characters and underscores  
* class\_label: species or other class name (e.g. “songsparrow\_aggression”) of the localized event  
* Start\_timestamp: onset time of event in ISO format `2025-05-20T10:00:00.000-05:00`  
* duration: event length in seconds  
* position: sound source position coordinate in meters: (utm x, utm y ,elevation, utm zone)  
* comments  
* \[optional\]: direction (degrees CW from N): orientation of animal when sound was produced

Can include files for other observation types, such as:

- Spot mapping  
- Point count survey  
- Transect survey  
- Focal follows

Include the cleaned data files and any supplemental metadata (typically as a .csv or .json). In the README.md file, describe how the data were generated (protocol, post-processing) and how to interpret the files in this folder (e.g. meaning of each column). If different species codes are used than in localized\_events.csv “class” column, make sure to include information on the codes and how to interpret them.
